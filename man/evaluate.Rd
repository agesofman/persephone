% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluate.R
\name{evaluate}
\alias{evaluate}
\alias{evaluate,ProgressModel-method}
\alias{evaluate,ProgressModelList-method}
\title{Evaluate model performance}
\usage{
evaluate(object, ...)

\S4method{evaluate}{ProgressModel}(object, ...)

\S4method{evaluate}{ProgressModelList}(object, ...)
}
\arguments{
\item{object}{an object of class \code{ProgressModel} or \code{ProgressModelList}.}

\item{...}{extra arguments.}
}
\value{
An object identical to \code{object}, with the \code{metrics} slot(s) altered.
}
\description{
Calculate the model performance metrics using leave-group-out (or Monte
Carlo) cross-validation.
}
\examples{
\dontrun{
# Create a Region object
library(cronus)
region <- Region(name = "nebraska", type = "us state",
                 div = c(country = "United States", state = "Nebraska"))

# Create a model
object1 <- new("ProgressBM",
               region = region,
               crop = "Corn",
               data = data_progress$Corn,
               formula = "CumPercentage ~ Time + agdd") # ProgressModel

# Create another model
object2 <- new("ProgressCLM",
               region = region,
               crop = "Soybeans",
               data = data_progress$Soybeans,
               formula = "Stage ~ Time + agdd + adayl") # ProgressModel

# Concatenate the models
object <- c(object1, object2) # ProgressModelList

# Fit
object <- fit(object)

# Plot
plot(object, cumulative = TRUE, seasons = 2002)

# Predict
predict(object, data_progress)

# Evaluate
object <- evaluate(object, maxsam = 100, seed = 1)
plot_metrics(object)

# Summarize
summary(object)

# Report
report(object, name = "example_report", path = getwd())
}
}
